{"data":{"wordpressPost":{"title":"The trials and tribulations of a large-ish dataset","content":"<p>Last week I wrote a little script in node.js.</p>\n<p>Its goal? GET ALL THE DATA!</p>\n<a href=\"http://nightowls.swizec.com/histogram/hours\"><img class=\"wp-image-5853 alignnone\" title=\"Commit time histogram\" src=\"http://swizec.com/blog/wp-content/uploads/2013/01/commit-histogram.png\" alt=\"Commit time histogram\" width=\"648\" height=\"207\" srcset=\"https://swizec.com/blog/wp-content/uploads/2013/01/commit-histogram.png 926w, https://swizec.com/blog/wp-content/uploads/2013/01/commit-histogram-300x95.png 300w\" sizes=\"(max-width: 648px) 100vw, 648px\" /></a>\n<p>The plan was to scrape a massive <a class=\"zem_slink\" title=\"Data set\" href=\"http://en.wikipedia.org/wiki/Data_set\" rel=\"wikipedia\" target=\"_blank\">dataset</a> off <a class=\"zem_slink\" title=\"GitHub\" href=\"http://github.com\" rel=\"homepage\" target=\"_blank\">Github</a> and do some analysis of programmers&#8217; working habits, at least the ones using Github that is. As such, the script was fairly simple &#8211; run a search query via the API, get page 1, 2, 3 &#8230;</p>\n<p>At 100 repositories per page with a round-trip of a few seconds and having to wait for an hour before being allowed to do more requests after every couple of hundred pages, the scraping job took the bigger part of a week. Occasionally the script would hang without getting a <em>403 error</em> so I changed the query. That fixed it.</p>\n<p>Sometimes it would crash &#8211; right after I&#8217;d gone to bed of course.</p>\n<p>On Saturday morning I had a mongo database with a list of <em>513,900</em> repositories on a small EC2 instance. They were not guaranteed to be unique.</p>\n<h2>When N becomes a problem</h2>\n<p>The next step was going through the list of repositories and fetching punchcard data for each.</p>\n<div id=\"attachment_5855\" style=\"width: 660px\" class=\"wp-caption alignnone\"><a href=\"http://swizec.com/blog/wp-content/uploads/2013/01/swizec-punchcard.png\"><img class=\" wp-image-5855 \" title=\"This is a punchcard\" src=\"http://swizec.com/blog/wp-content/uploads/2013/01/swizec-punchcard.png\" alt=\"This is a punchcard\" width=\"650\" height=\"213\" srcset=\"https://swizec.com/blog/wp-content/uploads/2013/01/swizec-punchcard.png 929w, https://swizec.com/blog/wp-content/uploads/2013/01/swizec-punchcard-300x98.png 300w\" sizes=\"(max-width: 650px) 100vw, 650px\" /></a><p class=\"wp-caption-text\">This is a punchcard</p></div>\n<p>A core saying of practical computer science goes something like <em>&#8220;Our O(N^5) algorithm sucks for large N, but N is usually less than 10&#8221;</em></p>\n<p>Although not a very big dataset by most means, at half a million, it was enough to thwart my original approach of using <a href=\"http://mongoosejs.com/\" target=\"_blank\">Mongoose</a> to handle my mongodb connection.</p>\n<p>While Mongoose was great for writing, it failed completely at reading. Not exactly certain where the problem was, but running this sort of code quickly made my EC2 instance run out of memory:</p>\n<pre lang=\"javascript\">models.repo.find({}, function (err, repos) {\r\n   repos.forEach(do_something);\r\n});</pre>\n<p>Where <em>models.repo</em> is the Schema for a repository.</p>\n<p>So I was forced to ditch the relative comforts afforded by Mongoose and read my data the old fashioned way:</p>\n<pre lang=\"javascript\">mongo.Db.connect(\r\n        format(\"mongodb://%s:%s/github-nightowls?w=1\", 'localhost', 27017),\r\n        function(err, _db) {\r\n\r\n            var repos = _db.collection('repos'),\r\n                punchcards = _db.collection('punchcards');\r\n\r\n            repos.find().each(function (err, repo) {\r\n// ...</pre>\n<p>A lot more work, but it doesn&#8217;t even show up as a blip on memory usage. My guess is that Mongoose was doing the wrong thing and didn&#8217;t stream data right out of the database into my <a class=\"zem_slink\" title=\"Callback (computer programming)\" href=\"http://en.wikipedia.org/wiki/Callback_%28computer_programming%29\" rel=\"wikipedia\" target=\"_blank\">callback function</a>, but was trying to keep it all in memory. Silly Mongoose.</p>\n<p>There was one more step! Ensuring I only fetch punchcard data for unique repositories.</p>\n<p>My first attempt was going through the data, adding each repository&#8217;s username/name combination to a redis set, then following that set when fetching the punchcards. Remember, sets guarantee each member only shows up once.</p>\n<p>Alas. After running a script for a few minutes &#8211; oh yes, did I mention it takes about 3 minutes just to read through the entire list of repositories? &#8211; it turns out the <a class=\"zem_slink\" title=\"Redis (data store)\" href=\"http://redis.io/\" rel=\"homepage\" target=\"_blank\">Redis</a> set was so massive my attempts at reading it were to no avail. Ran out of memory before even doing anything serious.</p>\n<p>This time I&#8217;m not sure it&#8217;s <a href=\"https://github.com/mranney/node_redis\" target=\"_blank\">node_redis</a>&#8216;s fault or Redis really cannot stream data out of sets as is reading them.</p>\n<p>Eventually the algorithm to fetch punchards worked like this:</p>\n<ol>\n<li>Read object from Mongo</li>\n<li>Check Redis set if object already processed</li>\n<li>Fetch punchard</li>\n<li>Store punchcard</li>\n<li>Add new member to Redis set marking we&#8217;ve done this</li>\n</ol>\n<p>Because of some inconsistencies in Github responses the first time I ran this, it crashed 5 minutes after I had gone to sleep.</p>\n<p>Next run went much better!</p>\n<p>Except the instance ran out of disk space after processing some 110k repositories and I only noticed after waiting 14 hours for everything to process. Kids, print your errors!</p>\n<p>Third time is a charm and on Monday morning I had all the punchcards in a roughly 6 gigabyte mongo database. All <em>504,015</em> of them.</p>\n<p>Luckily I had spent the previous day coding up a script calculating histograms &#8211; takes about <em>3 minutes</em> to munch through the dataset &#8211; and playing with d3.js to make it look good.</p>\n<p>In all, the dataset contains <em>164,509,270</em> commits. Only in punchcard form for now. This means I only have a 7&#215;24 grid of buckets saying how many commits happened at that time.</p>\n<p>Next step &#8211; finding a way to distinguish hobby and work projects to see if that affects the distribution of commit times. Wish me luck.</p>\n<p>PS: If you want the dataset, send me an email and we can arrange something</p>\n<p>PPS: for those interested, all the code for fetching <a href=\"https://github.com/Swizec/github-nightowls\" target=\"_blank\">my dataset is on github</a></p>\n<h6 class=\"zemanta-related-title\" style=\"font-size: 1em;\">Related articles</h6>\n<ul class=\"zemanta-article-ul zemanta-article-ul-image\" style=\"margin: 0; padding: 0; overflow: hidden;\">\n<li class=\"zemanta-article-ul-li-image zemanta-article-ul-li\" style=\"padding: 0; background: none; list-style: none; display: block; float: left; vertical-align: top; text-align: left; width: 84px; font-size: 11px; margin: 2px 10px 10px 2px;\"><a style=\"box-shadow: 0px 0px 4px #999; padding: 2px; display: block; border-radius: 2px; text-decoration: none;\" href=\"http://stackoverflow.com/questions/14164108/deploying-an-mvc4-c-sharp-application-to-azure-via-github-what-should-be-in-my\" target=\"_blank\"><img style=\"padding: 0; margin: 0; border: 0; display: block; width: 80px; max-width: 100%;\" src=\"http://i.zemanta.com/noimg_110_80_80.jpg\" alt=\"\" /></a><a style=\"display: block; overflow: hidden; text-decoration: none; line-height: 12pt; height: 80px; padding: 5px 2px 0 2px;\" href=\"http://stackoverflow.com/questions/14164108/deploying-an-mvc4-c-sharp-application-to-azure-via-github-what-should-be-in-my\" target=\"_blank\">Deploying an MVC4 C# application to Azure via GitHub. What should be in my .gitignore?</a></li>\n<li class=\"zemanta-article-ul-li-image zemanta-article-ul-li\" style=\"padding: 0; background: none; list-style: none; display: block; float: left; vertical-align: top; text-align: left; width: 84px; font-size: 11px; margin: 2px 10px 10px 2px;\"><a style=\"box-shadow: 0px 0px 4px #999; padding: 2px; display: block; border-radius: 2px; text-decoration: none;\" href=\"http://pypi.python.org/pypi/BigJob/0.4.122\" target=\"_blank\"><img style=\"padding: 0; margin: 0; border: 0; display: block; width: 80px; max-width: 100%;\" src=\"http://i.zemanta.com/138108576_80_80.jpg\" alt=\"\" /></a><a style=\"display: block; overflow: hidden; text-decoration: none; line-height: 12pt; height: 80px; padding: 5px 2px 0 2px;\" href=\"http://pypi.python.org/pypi/BigJob/0.4.122\" target=\"_blank\">BigJob 0.4.122</a></li>\n<li class=\"zemanta-article-ul-li-image zemanta-article-ul-li\" style=\"padding: 0; background: none; list-style: none; display: block; float: left; vertical-align: top; text-align: left; width: 84px; font-size: 11px; margin: 2px 10px 10px 2px;\"><a style=\"box-shadow: 0px 0px 4px #999; padding: 2px; display: block; border-radius: 2px; text-decoration: none;\" href=\"http://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas\" target=\"_blank\"><img style=\"padding: 0; margin: 0; border: 0; display: block; width: 80px; max-width: 100%;\" src=\"http://i.zemanta.com/noimg_96_80_80.jpg\" alt=\"\" /></a><a style=\"display: block; overflow: hidden; text-decoration: none; line-height: 12pt; height: 80px; padding: 5px 2px 0 2px;\" href=\"http://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas\" target=\"_blank\">&#8220;Large data&#8221; work flows using pandas</a></li>\n<li class=\"zemanta-article-ul-li-image zemanta-article-ul-li\" style=\"padding: 0; background: none; list-style: none; display: block; float: left; vertical-align: top; text-align: left; width: 84px; font-size: 11px; margin: 2px 10px 10px 2px;\"><a style=\"box-shadow: 0px 0px 4px #999; padding: 2px; display: block; border-radius: 2px; text-decoration: none;\" href=\"http://port3000.co.uk/imageoptimiser-post-mortem-of-a-github-bot\" target=\"_blank\"><img style=\"padding: 0; margin: 0; border: 0; display: block; width: 80px; max-width: 100%;\" src=\"http://i.zemanta.com/134901638_80_80.jpg\" alt=\"\" /></a><a style=\"display: block; overflow: hidden; text-decoration: none; line-height: 12pt; height: 80px; padding: 5px 2px 0 2px;\" href=\"http://port3000.co.uk/imageoptimiser-post-mortem-of-a-github-bot\" target=\"_blank\">imageoptimiser: Post mortem of a GitHub bot</a></li>\n</ul>\n<div class=\"zemanta-pixie\" style=\"margin-top: 10px; height: 15px;\"><a class=\"zemanta-pixie-a\" title=\"Enhanced by Zemanta\" href=\"http://www.zemanta.com/?px\"><img class=\"zemanta-pixie-img\" style=\"border: none; float: right;\" src=\"http://img.zemanta.com/zemified_e.png?x-id=b124a913-ba09-45d1-84c3-ff52995f95e1\" alt=\"Enhanced by Zemanta\" /></a></div>\n"},"site":{"siteMetadata":{"title":"Swizec Blog","subtitle":"Fetch Data From Local WP Install"}}},"pageContext":{"id":"beb0b7a9-1c52-5d56-9e4f-fa804c5f7c53"}}