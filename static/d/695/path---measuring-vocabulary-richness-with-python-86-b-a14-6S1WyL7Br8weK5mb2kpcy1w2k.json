{"data":{"wordpressPost":{"title":"Measuring vocabulary richness with python","content":"<div class=\"zemanta-img\" style=\"margin: 1em; display: block;\">\n<div style=\"width: 250px\" class=\"wp-caption alignright\"><a href=\"http://www.flickr.com/photos/28722563@N05/6192586527\"><img title=\"Pillowfights 51\" src=\"http://swizec.com/blog/wp-content/uploads/2011/09/6192586527_c36407222c_m7.jpg\" alt=\"Pillowfights 51\" width=\"240\" height=\"160\" /></a><p class=\"wp-caption-text\">Image by asterix611 via Flickr</p></div>\n</div>\n<p>In preparation for a blogpost I&#8217;m going to make some time this week I found myself wanting to somehow parametrize <a class=\"zem_slink\" title=\"Vocabulary\" href=\"http://en.wikipedia.org/wiki/Vocabulary\" rel=\"wikipedia\">vocabulary</a> richness in a piece of text.</p>\n<p><em style=\"font-size: 1.2em\">btw, Code at bottom ðŸ˜‰</em></p>\n<p>It&#8217;s an interesting problem because when you read something, it&#8217;s pretty easy to see when an author is using rich vocabulary, but trying to reduce this observation to a simple number turns out to be a bit of a brainfuck. It&#8217;s obviously somehow related to word frequencies and it seems obvious that what we want to measure is the distribution shape of word frequencies.</p>\n<p>Luckily googling around for about an hour turned up a clue. Way back in 1944 a statistician called <a href=\"http://en.wikipedia.org/wiki/Udny_Yule\">G.U. Yule</a> cracked this problem in a paper titled <em><a href=\"http://scholar.google.si/scholar?q=The+statistical+study+of+literary+vocabulary+&amp;hl=sl&amp;btnG=Iskanje\">The statistical study of literary vocabulary</a></em>. What he came up with is the so called Yule&#8217;s K characteristic.</p>\n<p>Wikipedia is scarce on these things, so we know we&#8217;re treading strange strange ground here. Persistent searching with google scholar turned up a version of the paper that wasn&#8217;t paywalled to oblivion. However since it was on google books it was missing some key pages.</p>\n<p>Luckily what looks like a random homework for R explains perfectly how to implement Yule&#8217;s K value:</p>\n<blockquote><p>A complementary way of assessing the vocabulary difficulty of texts is to measure their lexical richness. Two indices one could use are Yule&#8217;s K or Yule&#8217;s I. These two are defined as follows:<br />\n(1) Yule&#8217;s K = 10,000â‹…î‚žM 2âˆ’M 1 î‚ŸÃ·î‚ž M 1â‹…M 1î‚Ÿ<br />\n(2) Yule&#8217;s I = î‚žM 1â‹…M 1î‚ŸÃ·î‚žM 2âˆ’M 1 î‚Ÿ<br />\nwhere M1 is the number of all word forms a text consists of and M2 is the sum of the products of each observed frequency to the power of two and the number of word types observed with that frequency (cf. Oakes 1998:204). For example, if one word occurs three times and four words occur five times, M2=(1*32)+(4*52)=109. The larger Yule&#8217;s K, the smaller the diversity of the vocabulary (and thus, arguably, the easier the text). Since Yule&#8217;s I is based on the reciprocal of Yule&#8217;s K, the larger Yule&#8217;s I, the larger the diversity of the vocabulary (and thus, arguably, the more difficult the text).</p></blockquote>\n<p>Unfortunately I don&#8217;t have the link anymore. It was a seriously random pdf I found online, the title seems to be <em>&#8220;Quantitative <a class=\"zem_slink\" title=\"Corpus linguistics\" href=\"http://en.wikipedia.org/wiki/Corpus_linguistics\" rel=\"wikipedia\">corpus linguistics</a> with R: a practical introduction&#8221;</em></p>\n<p>In hopes this blogpost saves somebody a few hours of googling when trying to measure vocabulary richness, here&#8217;s my python implementation of Yule&#8217;s K characteristic (or rather its inverse, Yule&#8217;s I)</p>\n<pre lang=\"python\">from nltk.stem.porter import PorterStemmer\r\nfrom itertools import groupby\r\n\r\ndef words(entry):\r\n    return filter(lambda w: len(w) &gt; 0,\r\n                  [w.strip(\"0123456789!:,.?(){}[]\") for w in entry.split()])\r\n\r\ndef yule(entry):\r\n    # yule's I measure (the inverse of yule's K measure)\r\n    # higher number is higher diversity - richer vocabulary\r\n    d = {}\r\n    stemmer = PorterStemmer()\r\n    for w in words(entry):\r\n        w = stemmer.stem(w).lower()\r\n        try:\r\n            d[w] += 1\r\n        except KeyError:\r\n            d[w] = 1\r\n\r\n    M1 = float(len(d))\r\n    M2 = sum([len(list(g))*(freq**2) for freq,g in groupby(sorted(d.values()))])\r\n\r\n    try:\r\n        return (M1*M1)/(M2-M1)\r\n    except ZeroDivisionError:\r\n        return 0</pre>\n<p>For example the output of that function for this post is 21.6</p>\n<p>Just wish I knew how to make that middle part more functional-like. I don&#8217;t like having weird for loops strewn about my code like that.</p>\n<h6 class=\"zemanta-related-title\" style=\"font-size: 1em;\">Related articles</h6>\n<ul class=\"zemanta-article-ul\">\n<li class=\"zemanta-article-ul-li\"><a href=\"http://www.makeuseof.com/tag/improve-english-learn-words-vocabularycom/\">Improve Your English &amp; Learn New Words With Vocabulary.com</a> (makeuseof.com)</li>\n<li class=\"zemanta-article-ul-li\"><a href=\"http://annmic.wordpress.com/2011/09/20/vocabulary-building/\">Vocabulary Building</a> (annmic.wordpress.com)</li>\n<li class=\"zemanta-article-ul-li\"><a href=\"http://www.education.com/worksheets/vocabulary/\">Vocabulary Worksheets and Printables</a> (education.com)</li>\n</ul>\n<div class=\"zemanta-pixie\" style=\"margin-top: 10px; height: 15px;\"><a class=\"zemanta-pixie-a\" title=\"Enhanced by Zemanta\" href=\"http://www.zemanta.com/\"><img class=\"zemanta-pixie-img\" style=\"border: none; float: right;\" src=\"http://img.zemanta.com/zemified_e.png?x-id=6825c36b-0d62-4d69-a0eb-3b4d98c62cad\" alt=\"Enhanced by Zemanta\" /></a></div>\n"},"site":{"siteMetadata":{"title":"Swizec Blog","subtitle":"Fetch Data From Local WP Install"}}},"pageContext":{"id":"6557ea77-ba20-56cf-a580-332b9914cfed"}}