{"data":{"wordpressPost":{"title":"I suck at implementing neural networks in octave","content":"<p>A few days ago I implemented my first full <a class=\"zem_slink\" title=\"Neural network\" href=\"http://en.wikipedia.org/wiki/Neural_network\" rel=\"wikipedia\">neural network</a> in Octave. Nothing too major, just a three layer network recognising hand-written letters. Even though I <a title=\"I think I finally understand what a neural network is\" href=\"http://swizec.com/blog/i-think-i-finally-understand-what-a-neural-network-is/swizec/2891\">finally understood what a neural network is</a>, this was still a cool challenge.</p>\n<p>Yes, even despite having so much support from ml-class &#8230; they practically implement everything and just leave the cost and gradient functions up to you to implement. Then again, Octave provides tools for learning where you essentially just run a function, tell it where to find the cost and gradient function and give it some data.</p>\n<p>Then the magic happens.</p>\n<p>Getting the basic implementation to work is really simple since the formulas being used aren&#8217;t all that complex:</p>\n<div id=\"attachment_2931\" style=\"width: 543px\" class=\"wp-caption alignnone\"><a href=\"http://swizec.com/blog/wp-content/uploads/2011/11/Screen-Shot-2011-11-16-at-1.07.26-AM.png\"><img class=\"size-full wp-image-2931 \" title=\"Neural network cost function\" src=\"http://swizec.com/blog/wp-content/uploads/2011/11/Screen-Shot-2011-11-16-at-1.07.26-AM.png\" alt=\"Neural network cost function\" width=\"533\" height=\"194\" srcset=\"https://swizec.com/blog/wp-content/uploads/2011/11/Screen-Shot-2011-11-16-at-1.07.26-AM.png 761w, https://swizec.com/blog/wp-content/uploads/2011/11/Screen-Shot-2011-11-16-at-1.07.26-AM-300x109.png 300w\" sizes=\"(max-width: 533px) 100vw, 533px\" /></a><p class=\"wp-caption-text\">Neural network cost function</p></div>\n<div id=\"attachment_2932\" style=\"width: 660px\" class=\"wp-caption alignnone\"><a href=\"http://swizec.com/blog/wp-content/uploads/2011/11/Screen-Shot-2011-11-16-at-1.08.17-AM.png\"><img class=\"size-full wp-image-2932 \" title=\"Neural network backpropagation\" src=\"http://swizec.com/blog/wp-content/uploads/2011/11/Screen-Shot-2011-11-16-at-1.08.17-AM.png\" alt=\"Neural network backpropagation\" width=\"650\" height=\"302\" srcset=\"https://swizec.com/blog/wp-content/uploads/2011/11/Screen-Shot-2011-11-16-at-1.08.17-AM.png 928w, https://swizec.com/blog/wp-content/uploads/2011/11/Screen-Shot-2011-11-16-at-1.08.17-AM-300x139.png 300w\" sizes=\"(max-width: 650px) 100vw, 650px\" /></a><p class=\"wp-caption-text\">Neural network backpropagation</p></div>\n<p>Here&#8217;s the code I&#8217;ve come up with to get this working on a three layer network:</p>\n<pre lang=\"matlab\">function [J grad] = nnCostFunction(nn_params, ...\r\n                                   input_layer_size, ...\r\n                                   hidden_layer_size, ...\r\n                                   num_labels, ...\r\n                                   X, y, lambda)\r\n%NNCOSTFUNCTION Implements the neural network cost function for a two layer\r\n%neural network which performs classification\r\n%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...\r\n%   X, y, lambda) computes the cost and gradient of the neural network. The\r\n%   parameters for the neural network are \"unrolled\" into the vector\r\n%   nn_params and need to be converted back into the weight matrices.\r\n%\r\n%   The returned parameter grad should be a \"unrolled\" vector of the\r\n%   partial derivatives of the neural network.\r\n%\r\n\r\n% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\r\n% for our 2 layer neural network\r\nTheta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\r\n                 hidden_layer_size, (input_layer_size + 1));\r\n\r\nTheta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\r\n                 num_labels, (hidden_layer_size + 1));\r\n\r\n% Setup some useful variables\r\nm = size(X, 1);\r\n\r\n% You need to return the following variables correctly\r\nJ = 0;\r\nTheta1_grad = zeros(size(Theta1));\r\nTheta2_grad = zeros(size(Theta2));\r\n\r\nyy = zeros(size(y),num_labels);\r\nfor i=1:size(X)\r\n  yy(i,y(i)) = 1;\r\nend\r\n\r\nX = [ones(m,1) X];\r\n% cost\r\nfor  i=1:m\r\n  a1 = X(i,:);\r\n  z2 = Theta1*a1';\r\n  a2 = sigmoid(z2);\r\n  z3 = Theta2*[1; a2];\r\n  a3 = sigmoid(z3);\r\n\r\n  J += -yy(i,:)*log(a3)-(1-yy(i,:))*log(1-a3);\r\nend\r\n\r\nJ /= m;\r\n\r\nJ += (lambda/(2*m))*(sum(sum(Theta1(:,2:end).^2))+sum(sum(Theta2(:,2:end).^2)));\r\n\r\nt=1;\r\nfor t=1:m\r\n  % forward pass\r\n  a1 = X(t,:);\r\n  z2 = Theta1*a1';\r\n  a2 = [1; sigmoid(z2)];\r\n  z3 = Theta2*a2;\r\n  a3 = sigmoid(z3);\r\n\r\n  % backprop\r\n  delta3 = a3-yy(t,:)';\r\n  delta2 = (Theta2'*delta3).*[1; sigmoidGradient(z2)];\r\n  delta2 = delta2(2:end);\r\n\r\n  Theta1_grad = Theta1_grad + delta2*a1;\r\n  Theta2_grad = Theta2_grad + delta3*a2';\r\nend\r\n\r\nTheta1_grad = (1/m)*Theta1_grad+(lambda/m)*[zeros(size(Theta1, 1), 1) Theta1(:,2:end)];\r\nTheta2_grad = (1/m)*Theta2_grad+(lambda/m)*[zeros(size(Theta2, 1), 1) Theta2(:,2:end)];\r\n\r\n% Unroll gradients\r\ngrad = [Theta1_grad(:) ; Theta2_grad(:)];\r\n\r\nend</pre>\n<p>This then basically gets pumped into the <em>fmincg</em>Â function and on the other end a result pops out.</p>\n<p>Now, I&#8217;ve managed to vectorize this thing to the edge of my capabilities. But I know it&#8217;s still just matrix multiplication so I know for a fact it should be possible to vectorize even further. Anyone know how to do that?</p>\n<p>Also, if you know of a cool way to generalize the algorithm so it would work on bigger networks, I&#8217;d love to hear about that as well!</p>\n<h6 class=\"zemanta-related-title\" style=\"font-size: 1em;\">Related articles</h6>\n<ul class=\"zemanta-article-ul\">\n<li class=\"zemanta-article-ul-li\"><a href=\"http://joeyjardins.wordpress.com/2011/10/22/unique-neural-networks-make-leo-trader-pro-a-unique-and-profitable-forex-trading-system/\">Unique Neural Networks Make Leo Trader Pro A Unique And Profitable Forex Trading System</a> (joeyjardins.wordpress.com)</li>\n<li class=\"zemanta-article-ul-li\"><a href=\"http://heuristically.wordpress.com/2011/11/11/train-neural-network-in-r-predict-in-sas/\">Train neural network in R, predict in SAS</a> (heuristically.wordpress.com)</li>\n<li class=\"zemanta-article-ul-li\"><a href=\"http://vql89.wordpress.com/2011/10/26/training-of-neural-network/\">Training of Neural Network</a> (vql89.wordpress.com)</li>\n</ul>\n<div class=\"zemanta-pixie\" style=\"margin-top: 10px; height: 15px;\"><a class=\"zemanta-pixie-a\" title=\"Enhanced by Zemanta\" href=\"http://www.zemanta.com/\"><img class=\"zemanta-pixie-img\" style=\"border: none; float: right;\" src=\"http://img.zemanta.com/zemified_e.png?x-id=2cd7a0e7-5dea-4e23-8cd7-086d494a8ef1\" alt=\"Enhanced by Zemanta\" /></a></div>\n"},"site":{"siteMetadata":{"title":"Swizec Blog","subtitle":"Fetch Data From Local WP Install"}}},"pageContext":{"id":"b6755a40-6f82-59b5-b813-ecc58b42b906"}}